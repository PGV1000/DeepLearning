<a href="https://colab.research.google.com/github/DianaShramchenko/works/blob/main/6.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
# **Практическая работа №6. Проектирование автоэнкодеров для решения прикладных задач**

[**Ссылка на код с пары**](https://colab.research.google.com/drive/1KfCEQly5k0PUdCR1w-my2-QmYfFexdvK?usp=sharing)
### - **N.B.: Во всех заданиях, графический интерфейс для взаимодействия с обученной моделью, реализуется с помощью Gradio!**

### - **Для каждого задания в графическом интерфейсе должно быть встроено минимум 3 примера (sample)**
# Задание №1. Реализуйте автоэнкодер для колоризации чёрно-белых изображений

* **Чем больше объектов разных классов будет в исходном датасете, тем универсальнее будет работа обученной Вами модели**

> Например, если в Вашем датасете только кошки, то все объекты кроме кошек будут колоризоваться некорректно. Соответственно следует очень тщательно подойти к выбору датасета.

  * Для решения данной задачи может подойти датасет [CIFAR-100](https://www.kaggle.com/datasets/fedesoriano/cifar100), но его минус заключается в том, что разрешение изображений довольно низкое

* Хорошим вариантом будет использование датасета [ImageNet](https://paperswithcode.com/dataset/imagenet), но ввиду его объёмности, процесс обучения займет большое количество времени, поэтому Вы можете использовать одну из его [сокращенных версий](https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000)

 **Конечный выбор датасета осуществляется по Вашему желанию, учитывая рекомендации приведенные выше.**

from keras.layers import Conv2D, UpSampling2D, Input
from keras.models import Sequential, Model
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import img_to_array, load_img
from skimage.color import rgb2lab, lab2rgb, gray2rgb, rgb2gray
from skimage.transform import resize
from skimage.io import imsave
from matplotlib import pyplot as plt
import numpy as np
import tensorflow as tf
import keras
import cv2
import os
from keras.utils import image_dataset_from_directory
from imutils import paths
import pathlib
import random
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model
%%capture
DATADIR = 'Dataset'
if os.path.isdir(DATADIR)>0:
    print ('Data directory already exists and it is not empty, miss downloading')
else:
    print ('Data directory does not exists or it is empty, download the dataset')
    !gdown --id 1SaAEAxiqU4sSXsFEja1JRARpXCGQIkJN #Скопировать ID из ссылки в гугл диске
    !unzip Dataset.zip
# инициализируем данные и метки
print("[INFO] loading images...")
data = []
labels = []
from google.colab import drive
drive.mount('/content/drive')
# Собираем список путей к каждому изображению и перемешиваем их
imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/content/imag_mini/")))
print(imagePaths[70].split(os.path.sep)[-2])
random.shuffle(imagePaths)
imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/content/imag_mini/")))
print(f"Количество найденных изображений: {len(imagePaths)}")
data_dir = pathlib.Path("/content/drive/MyDrive/content/imag_mini/")
print(data_dir)
def preprocess(array):
    """
    Нормализует предоставленный массив и преобразует его в соответствующий формат.

    """

    array = array.astype("float32") / 255.0
    array = np.reshape(array, (len(array), 28, 28, 1))
    return array
#В качестве кодировщика используем предобученную модель с архитектурой VGG16
from keras.applications.vgg16 import VGG16
vggmodel = VGG16() # Импортируем предобученную модель VGG16 с параметрами по умолчанию
newmodel = Sequential() # Создаём пустую модель

for i, layer in enumerate(vggmodel.layers): # Заполняем пустую модель слоями из предобученной модели VGG16
    if i<19:          # Только до 19-го слоя, чтобы включить только слои, используемые для извлечения признаков
      newmodel.add(layer)

newmodel.summary()
for layer in newmodel.layers:
  layer.trainable=False   # Мы не хотим снова обучать эти слои, поэтому False.
path = '/content/drive/MyDrive/content/imag_mini/'
train_datagen = ImageDataGenerator(rescale=1. / 255)
train = train_datagen.flow_from_directory(path, target_size=(224, 224), batch_size=5, class_mode=None)

print(train)
print(train)
print(train[0].shape)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
from skimage.color import rgb2lab
import os

path = '/content/drive/MyDrive/content/imag_mini/'
train_datagen = ImageDataGenerator(rescale=1. / 255)
train = train_datagen.flow_from_directory(path, target_size=(224, 224), batch_size=5, class_mode=None)

print(train)
print(train[0].shape)

def process_batch(batch):
    X_batch = []
    Y_batch = []
    for img in batch:
        lab = rgb2lab(img)
        X_batch.append(lab[:,:,0])
        Y_batch.append(lab[:,:,1:] / 128)
    X_batch = np.array(X_batch)
    Y_batch = np.array(Y_batch)
    X_batch = X_batch.reshape(X_batch.shape + (1,))
    return X_batch, Y_batch

# Перемещение цикла for вне функции process_batch
for i in range(len(train)):
    batch = train.next()
    X, Y = process_batch(batch)
    # Здесь можно сохранить или использовать X и Y по мере необходимости
    print(f"Processed batch {i + 1}")
#Проверка преобразования первого изображения в батче
original_img = batch[0]
lab_img = rgb2lab(original_img)

print("Original image (RGB) pixel [0,0]:", original_img[0, 0])
print("Converted image (LAB) pixel [0,0]:", lab_img[0, 0])

# Визуализация изображений
def show_images(rgb_img, lab_img):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    axes[0].imshow(rgb_img)
    axes[0].set_title("Original RGB Image")
    axes[0].axis('off')

    lab_img_visual = (lab_img + [0, 128, 128]) / [100, 255, 255]  # Нормализация для визуализации
    axes[1].imshow(lab_img_visual)
    axes[1].set_title("LAB Image (Normalized)")
    axes[1].axis('off')

    plt.show()

show_images(original_img, lab_img)
# Создается пустой список для хранения признаков, извлеченных с помощью VGG
vggfeatures = []

# Перебираются образцы в наборе данных X
for i, sample in enumerate(X):
  # Конвертируется одноканальное изображение в трехканальное (RGB)
  sample = gray2rgb(sample)
  # Изменяется форма образца для соответствия входу сети VGG16
  sample = sample.reshape((1,224,224,3))
  # Производится предварительная обработка образца для сети VGG16
  sample = keras.applications.vgg16.preprocess_input(sample)
  # Получается предсказание модели для обработанного образца
  prediction = newmodel.predict(sample)
  # Изменяется форма предсказания для соответствия выходу сети VGG16
  prediction = prediction.reshape((7,7,512))
  # Добавляется предсказание в список признаков
  vggfeatures.append(prediction)

# Преобразуется список признаков в NumPy массив
vggfeatures = np.array(vggfeatures)
# Выводится форма массива признаков
print(vggfeatures.shape)
#Decoder
model = Sequential()

model.add(Conv2D(256, (3,3), activation='relu', padding='same', input_shape=(7,7,512)))
model.add(Conv2D(128, (3,3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(64, (3,3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3,3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(16, (3,3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))
model.add(UpSampling2D((2, 2)))
model.summary()


model.compile(optimizer='Adam', loss='mse' , metrics=['accuracy'])

model.fit(vggfeatures, Y, verbose=1, epochs=1000, batch_size=256)

model.save('/content/colorize_autoencoder_VGG16.h5')
model = tf.keras.models.load_model('/content/colorize_autoencoder_VGG16.h5',
                                   custom_objects=None,
                                   compile=True)
#Тестирование обученной модели на пользовательских данных

# Задается путь к изображениям для обработки
testpath = '/content/drive/MyDrive/content/imag_mini/val/'

# Получается список файлов в указанной директории
files = os.listdir(testpath)

# Перебираются файлы в директории
for idx, file in enumerate(files):
    # Изображение преобразуется в массив и нормализуется
    test = img_to_array(load_img(os.path.join(testpath, file)))
    test = resize(test, (224, 224), anti_aliasing=True)
    test *= 1.0 / 255

    # Изображение конвертируется из RGB в Lab
    lab = rgb2lab(test)
    l = lab[:, :, 0]

    # Канал L преобразуется обратно в RGB
    L = gray2rgb(l)
    L = L.reshape((1, 224, 224, 3))

    # Изображение подготавливается для модели VGG16
    L = keras.applications.vgg16.preprocess_input(L)

    # Получается предсказание модели для канала L
    vggpred = newmodel.predict(L)
    ab = model.predict(vggpred)

    # Каналы a и b умножаются на 128 для преобразования обратно в цветовое пространство LAB
    ab = ab * 128

    # Создается новое изображение с каналами L, a и b
    cur = np.zeros((224, 224, 3))
    cur[:, :, 0] = l
    cur[:, :, 1:] = ab

    # Преобразуем изображение в формат uint8 перед сохранением
    cur_rgb = lab2rgb(cur)
    cur_rgb_uint8 = (cur_rgb * 255).astype(np.uint8)

    # Изображение сохраняется на диск
    imsave('/content/' + str(idx) + ".jpg", cur_rgb_uint8)

#Вывод результатов:
from google.colab.patches import cv2_imshow
import numpy as np


imo_1 = cv2.imread("/content/0.jpg")
im_1 = cv2.imread("/content/1.jpg")
imo_2 = cv2.imread("/content/2.jpg")
im_2 = cv2.imread("/content/3.jpg")
imo_3 = cv2.imread("/content/4.jpg")
im_3 = cv2.imread("/content/5.jpg")
imo_4 = cv2.imread("/content/6.jpg")
im_4 = cv2.imread("/content/7.jpg")

pred_1 = np.concatenate((imo_1, im_1), axis=1)
pred_2 = np.concatenate((imo_2, im_2), axis=1)
pred_3 = np.concatenate((imo_3, im_3), axis=1)
pred_4 = np.concatenate((imo_4, im_4), axis=1)

cv2_imshow(pred_4)
cv2_imshow(pred_1)
cv2_imshow(pred_2)
cv2_imshow(pred_3)
# Задание №2. Реализуйте автоэнкодер для удаления шума на однотипных изображениях

1.  Подберите датасет, состоящий из однотипных изображений, которые в реальной жизни часто подвержены зашумлению, например спутниковые снимки, ночные фотографии и т.д.

2.  Затем примените к ним операцию зашумления и обучите модель. Также учитывайте тип шума, который Вы применяете. Он должен быть приближен к естественному.

Хороший пример реализации подобной задачи: https://www.kaggle.com/code/michalbrezk/denoise-images-using-autoencoders-tf-keras
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras import layers
from tensorflow.keras.models import Model
from keras.datasets import cifar100
%%capture
DATADIR = 'Dataset'
if os.path.isdir(DATADIR)>0:
    print ('Data directory already exists and it is not empty, miss downloading')
else:
    print ('Data directory does not exists or it is empty, download the dataset')
    !gdown --id 1SaAEAxiqU4sSXsFEja1JRARpXCGQIkJN #Скопировать ID из ссылки в гугл диске
    !unzip Dataset.zip
from google.colab import drive
drive.mount('/content/drive')
# Собираем список путей к каждому изображению и перемешиваем их
imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/content/forest/")))
print(imagePaths[10].split(os.path.sep)[-2])
random.shuffle(imagePaths)
imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/content/forest/")))
print(f"Количество найденных изображений: {len(imagePaths)}")
data_dir = pathlib.Path("/content/drive/MyDrive/content/forest/")
def preprocess(images, shape):
  """
  Нормализует предоставленный массив и преобразует его в соответствующий формат.

  """

  proc_images = []
  for image in images:
      proc_image = cv2.imread(image)
      proc_image = cv2.resize(proc_image,  (shape[0], shape[1]))
      proc_images.append(proc_image)

  proc_images = np.array(proc_images).astype("float32") / 255.0
  return proc_images


def noise(array):
    """
    Добавляет случайный шум к каждому изображению в предоставленном массиве.

    """

    noise_factor = 0.1
    noisy_array = array + noise_factor * np.random.normal(
        loc=0.0, scale=1.0, size=array.shape
    )

    return np.clip(noisy_array, 0.0, 1.0)


def display(array1, array2):
    """
    Отображает десять случайных изображений из каждого из предоставленных массивов.
    """
    n = 10
    indices = np.random.randint(len(array1), size=n)
    images1 = array1[indices, :]
    images2 = array2[indices, :]

    plt.figure(figsize=(20, 4))
    for i, (image1, image2) in enumerate(zip(images1, images2)):
        ax = plt.subplot(2, n, i + 1)
        plt.imshow(image1)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

        ax = plt.subplot(2, n, i + 1 + n)
        plt.imshow(image2)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

    plt.show()
# # инициализируем данные и метки
data = []
# # Собираем список путей к каждому изображению и перемешиваем их
imagePaths = sorted(list(paths.list_images(data_dir)))
random.shuffle(imagePaths)
img_h = 96 # Высота изображения в пикселях
img_w = 96 # Ширина изображения в пикселях
img_ch = 3 # Количество каналов в изображении
# Нормализация и изменение формы данных
data = preprocess(imagePaths, (img_h,img_w,img_ch))
# Создание копии данных с добавленным шумом
noisy_data = noise(data)

# Отображение данных выборки и его версии с добавленным шумом
display(data, noisy_data)
from sklearn.model_selection import train_test_split
# Поскольку для кодирования и декодирования нам нужны только изображения из набора данных, мы
# не будем использовать метки классов

train_data, val_data, train_noisy_data, val_noisy_data = train_test_split(data, noisy_data, test_size=0.2, random_state=42)


print("Train Data Shape:", train_data.shape)
print("Validation Data Shape:", val_data.shape)
print("Train Noisy Data Shape:", train_noisy_data.shape)
print("Validation Noisy Data Shape:", val_noisy_data.shape)

# Поскольку для кодирования и декодирования нам нужны только изображения из набора данных, мы
# не будем использовать метки классов
#train_data = load_images_from_directory(data_dir)
#test_data = load_images_from_directory(data_dir)
# Нормализация и изменение формы данных
#train_data = preprocess(train_data, (len(train_data), 28, 28, 1))
#test_data = preprocess(test_data, (len(train_data), 28, 28, 1))

# Создание копии данных с добавленным шумом
#noisy_train_data = noise(train_data)
#noisy_test_data = noise(test_data)

# Отображение данных выборки и его версии с добавленным шумом
#display(train_data, noisy_train_data)
input = layers.Input(shape=(96, 96, 3))

# Encoder
x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(input)
x = layers.MaxPooling2D((2, 2), padding="same")(x)
x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(x)
x = layers.MaxPooling2D((2, 2), padding="same")(x)

# Decoder
x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
x = layers.Conv2D(3, (3, 3), activation="sigmoid", padding="same")(x)

# Autoencoder
autoencoder = Model(input, x)
autoencoder.compile(optimizer="adam", loss="binary_crossentropy")
autoencoder.summary()
autoencoder.fit(
    x=train_data,
    y=train_data,
    epochs=50,
    batch_size=15,
    shuffle=True,
    validation_data=(val_data, val_data),
)
predictions = autoencoder.predict(val_data)
display(val_data, predictions)
#Использование автоэнкодера для удаления шума на изображении
autoencoder.fit(
    x=train_noisy_data,
    y=train_data,
    epochs=50,
    batch_size=15,
    shuffle=True,
    validation_data=(val_noisy_data, val_data),
)
predictions = autoencoder.predict(val_noisy_data)
display(val_noisy_data, predictions)
# Задание №.3 Реализуйте автоэнкодер для улучшения качества изображения путём увеличения его разрешения (апскейлинг-[определение](https://dic.academic.ru/dic.nsf/ruwiki/346555))

1. На входной слой нейронной сети подаётся изображение с размерностью (256, 256,3) - X_test. В результате работы нейронной сети на выходном слое должно получиться изображение (512, 512, 3) - Y_test. В итоге мы получаем двукратный апскейл исходного изображения.

2. Датасет собираете из изображений, разрешение которых выше эталонного (512,512,3). Затем преобразуете их в указанные размерности и формируете из них обучающую и проверочную выборку.

3. Обучите модель. Отобразите графики обучения

4. Подумайте, каким образом можно будет адаптировать модель вашей нейронной сети для двухратного апскейла изображения с любой размерностью, большей, чем (256,256,3), с полным или частичным сохранением его исходных пропорций. Например: подаём на вход изображение с разрешением (1920, 1080, 3) - получаем изображение с разрешением (3840, 2160, 3), т.е. с полным сохранением исходных пропорций или (1792, 1024, 3) с частичным сохранением исходных пропорций.

> Подсказка: Для этого можно реализовать алгоритм предварительной обработки исходного изображения, перед подачей его в нейронную сеть
# Импортируем LabelBinarizer для преобразования текстовых меток в векторы
# (например, "кошка" -> [1, 0, 0], "собака" -> [0, 1, 0], "панда" -> [0, 0, 1])
from sklearn.preprocessing import LabelBinarizer

# Импортируем train_test_split для разделения данных на обучающую и тестовую выборки
from sklearn.model_selection import train_test_split

# Импортируем classification_report для оценки качества классификации
from sklearn.metrics import classification_report

# Импортируем Sequential для создания последовательной модели нейронной сети
from keras.models import Sequential

# Импортируем Dense для создания плотных слоев нейронной сети
from keras.layers import Dense, Conv2D, Flatten

# Импортируем SGD и Adam для оптимизации обучения нейронной сети
from keras.optimizers import SGD, Adam

# Импортируем paths из imutils для удобной работы с путями к изображениям
from imutils import paths

# Импортируем pyplot из matplotlib для визуализации данных
import matplotlib.pyplot as plt

# Импортируем numpy для работы с массивами данных
import numpy as np

# Импортируем random для генерации случайных чисел
import random

# Импортируем pickle для сериализации и десериализации данных
import pickle

# Импортируем cv2 для работы с изображениями
import cv2

# Импортируем os для работы с файловой системой
import os

# Импортируем cv2_imshow из google.colab.patches для отображения изображений в Colab
from google.colab.patches import cv2_imshow

from tensorflow import keras
from google.colab import files
from io import BytesIO
from PIL import Image
from urllib.request import urlopen
from imutils import paths
import tensorflow as tf
from google.colab import drive
drive.mount('/content/drive')
# Собираем список путей к каждому изображению и перемешиваем их
imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/content/anim/")))
print(imagePaths[10].split(os.path.sep)[-2])
random.shuffle(imagePaths)
imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/content/anim/")))
print(f"Количество найденных изображений: {len(imagePaths)}")
data_dir = pathlib.Path("/content/drive/MyDrive/content/anim/")
def load_images(imagePaths, compression_size = 256):
  data = []
  half_length = len(imagePaths) // 4
  for imagePath in imagePaths[:half_length]:
    # Загрузка изображения
    image = cv2.imread(imagePath)
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    image = cv2.resize(image, (compression_size, compression_size))

    # Добавление изображения в список
    data.append(image)

  return data

data = load_images(imagePaths, 256)
data = np.array(data, dtype="float") / 255.0
def load_orig(imagePaths, compression_size = 512):
  data = []
  half_length = len(imagePaths) // 4
  for imagePath in imagePaths[:half_length]:
    # Загрузка изображения
    image = cv2.imread(imagePath)
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    image = cv2.resize(image, (compression_size, compression_size))
    # Добавление изображения в список
    data.append(image)

  return data
orig = load_orig(imagePaths, 512)
orig = np.array(orig, dtype="float") / 255.0
import matplotlib.pyplot as plt

for i in range(1, 10):
    plt.subplot(3, 3, i)
    plt.imshow(orig[i], cmap=plt.get_cmap('gray'))

plt.show()

for i in range(1, 10):
    plt.subplot(3, 3, i)
    plt.imshow(data[i], cmap=plt.get_cmap('gray'))

plt.show()
(trainX, valX) = train_test_split(data, test_size=0.25, random_state=28)
(origTrainX, origValX) = train_test_split(orig, test_size=0.25, random_state=28)
print(trainX)
print(valX)
print(origTrainX)
print(origValX)
# Encoder
from keras.layers import Input, Conv2D, UpSampling2D
from keras.optimizers import Adam
from keras import layers
from keras.models import Model


# Гиперпараметры
input_shape = (256, 256, 3)
output_shape = (512, 512, 3)
batch_size = 15
epochs = 50

# Создание модели автоэнкодера
input_img = Input(shape=input_shape)
x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D()(x)
decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)

autoencoder3 = Model(input_img, decoded)
autoencoder3.compile(optimizer='adam', loss='mse')
#обучаем модель
H = autoencoder3.fit(trainX,
                origTrainX,
                epochs=epochs,
                batch_size=batch_size,
                shuffle=True,
                validation_data=(valX, origValX))
# строим график обучения
plt.plot(H.history['loss'], label='Train Loss')
plt.plot(H.history['val_loss'], label='Validation Loss')
plt.title('Model Training')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
import matplotlib.pyplot as plt

for i in range(1, 10):
    plt.subplot(3, 3, i)
    plt.imshow(orig[i], cmap=plt.get_cmap('gray'))

plt.show()

for i in range(1, 10):
    plt.subplot(3, 3, i)
    plt.imshow(data[i], cmap=plt.get_cmap('gray'))

plt.show()

predictions = autoencoder3.predict(data)

for i in range(1, 10):
    plt.subplot(3, 3, i)
    plt.imshow(predictions[i], cmap=plt.get_cmap('gray'))

plt.show()
